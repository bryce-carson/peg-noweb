\chapter{Parsing project nowebs}\label{parsing}
This chapter covers the parsing of the Noweb tool syntax produced when
[[whyse]] executes a WHYSE project's defined shell script to generate the
tool syntax.

The \textsc{peg} package for \textsc{Emacs} provides automatic parser
generation from a formal PEG grammar. The grammar is based off of the
description of the tool syntax given in the CITE ramsey1992% \cite[Noweb Hacker's Guide]{ramsey1992}
.

\section{PEG rules}\label{rules}
Every character of an input text to be parsed by parsing expressions in
a PEG must be defined in terminal rules of the formal grammar.
%% NOTE: to match the new line character in a text stream, the string literal
%% "\n" must be included. The (eol) PEG rule /tests/ for the end of line by
%% guarding against unintended evaluation of the boolean return value of the
%% standard Emacs Lisp (eolp). To test if point is at the end of a line, use
%% (eol); to match the end of line, and permit parsing the next line of input,
%% include the string literal "\n".
The root rule in the grammar for Noweb tool syntax is named [[noweb]], so
[[with-peg-rules]] in scope a match using the the root rule [[noweb]] is
attempted against the current buffer. The current buffer contains the tool
syntax produced by the project shell script.

To aide in development, packaging [[<<PEG rules>>]] as an independent
peg package extension may force me to think of how the parse tree may be used.
Writing a useful interface to that parse tree before continuing on any other
adventure in WHYSE-land may help me complete this project.

Let's then create another Emacs LISP package (the modules for whyse.el have
already been defined).

<<peg-noweb.el>>=
;;; peg-noweb.el --- PEG ruleset for the Noweb Tool Syntax -*- lexical-binding: nil -*-

;; Copyright Â© 2025 Bryce Carson

;; Author: Bryce Carson <bcars268@mtroyal.ca>
;; Created 2025-09-01
;; Keywords: tools tex hypermedia peg
;; URL: https://github.com/bryce-carson/peg-noweb

;; This file is not part of GNU Emacs.

<<Licensing and copyright>>
;;; Commentary:
;; This file defines a PEG ruleset for Noweb's tool syntax, which is used to
;; extend Noweb with UNIX tools like AWK or sed.

;;; Code:
(define-peg-ruleset noweb
  <<PEG rules>>)

(provide 'peg-noweb)

;; Local Variables:
;; mode: emacs-lisp
;; no-byte-compile: t
;; no-native-compile: t
;; End:
@

The grammar of Noweb's tool syntax can be sectioned into five pieces, each
piece covering some aspect of parsing.

<<PEG rules>>=
<<high-level Noweb tool syntax structure>>
<<files and their paths>>
<<chunks and their boundaries>>
<<quotations>>
<<keyword definitions>>
<<meta rules>>
@

The [[noweb]] rule is the root expression which will match an instance of the
Noweb tool syntax. Erroneous or incomplete texts will not match.

As stated, the [[noweb]] rule defines the root expression---or starting
expression---for the grammar. The tool syntax of Noweb is a list
of one or more files, which are each composed of at least one chunk.
Ergo, the following [[<<high-level Noweb tool syntax structure>>]] is
defined.

<<high-level Noweb tool syntax structure>>=
;;; Overall Noweb structure
(noweb (bob) (not header) (+ file) (not trailer) (eob))
@

It is, more or less, an error for filters if the header or trailer keywords
appear in the text being parsed. These filters are irrelevant to any earlier
filter, and only matter for the final back-ends (\TeX{}, \LaTeX{}, or HTML) that
produce human-readable documenation. Normally a filter should ignore anything
that it doesn't manipulate, but here I decided to fail-safe and make it an
error. The grammar is designed with the [[--delay]] option in mind.

The grammar needs to address the fact that the syntax of the Noweb tool
format is highly line-oriented, given the influence of AWK on the design
and usage of Noweb (a historical version was entirely implemented in
AWK). The following [[<<meta rules>>]] define rules which organize the
constructs of a line-oriented, or data-oriented, syntax.

<<meta rules>>=
;; Helpers
(nl (eol) "\n")
(!eol (+ (not "\n") (any)))
(spc " ")
@

TODO: Review the following paragraph and rephrase it.

With the [[<<meta rules>>]] enabling easier definitions of what a given
``keyword'' looks like, the /concept of a file/ can be defined. A file
is ``anything that looks like a file to Noweb'';
%% TODO: verify the functionality of:
%% -Rone.el -Rtwo.el -Rthree.el#, or that of
%% -Rone.el,two.el,three.el,four,five.h,six.c#
files may be tangled from the noweb sources by specifying their root
chunk names on the command-line. However, by default, only the chunk
named ``*'' (it's chunk header is [[@<<*>>]]) is tangled when no
specific root chunk is given on the command line.

TODO: Write about the need for the overall document to be separate from
the one-or-more files specified in the document. Exempli gratia: the
current document, contained in \whysenw{} contains two files,
though they are separately tangled: \whyseel{} and
\texttt{test-parser-with-temporary-buffer.el}. If these two files were
tangled at the same time, such that the output file discovery ability of
Noweb was used, then the there would be more than one file in the
intermediate tool syntax, but still a single preceeding documentation
chunk before the first file, and a single succeeding documentation chunk
after the last file.

<<files and their paths>>=
;; Technically, file is a tagging keyword, but that classification only
;; makes sense in the Hacker's Guide to Noweb, not in the parser for the
;; tool syntax.
(file (bol) "@file" spc (substring path) nl
      (action (setq w--file-current-line 0))
      (list (and (+ chunk)
                 (list (or (and x-chunks i-identifiers)
                           (and i-identifiers x-chunks))))
            ;; Trailing documentation chunk and new-lines after the xref
            ;; and index.
            (opt chunk)
            (opt (+ nl)))
      `(path chunk-list -- (cons path chunk-list)))
(path (opt (or ".." ".")) (* path-component) file-name)
(path-component (and path-separator (+ [word])))
;; Perhaps I should instead `regexp-quote' the result of
;; `f-path-separator' rather than hard-code the UNIX-like path
;; separator. Doesn't Windows support automatic translation of UNIX-like
;; paths to Windows-paths on the current drive now?
(path-separator ["\\/"])
(file-name (+ (or [word] ".")))
@

NOTE: Writing PEXes for matching file names was the most difficult part
I have encountered so far, as it has forced me to understand that a
first reading of documentation is usually not sufficient to understand a
complex library in an area of programming I have not practiced in before
(language parsing).

Because chunks must not overlap, but can nest, the beginnings of chunks
need to be pushed to the parsing stack and the end of a chunk needs to
be popped off of it. The stack pushing operations in [[kind]] and
[[ordinal]] delimit chunks by their kinds and number, and the stack
actions in the [[end]] rule check that the chunk-related tokens on the
stack are balanced.

<<chunks and their boundaries>>=
(chunk begin (list (* chunk-contents)) end)
(begin (bol) "@begin" spc kind spc ordinal (eol) nl
       (action (if (string= (cl-second peg--stack) "code")
                   (setq w--peg-parser-within-codep t))))
(end (bol) "@end" spc kind spc ordinal (eol) nl
     (action
      (setq w--peg-parser-within-codep nil))
     ;; The stack grows down and the heap grows up,
     ;; that's the yin and yang of the computer thang
     `(kind-one
       ordinal-one
       keywords
       kind-two
       ordinal-two
       --
       (if (and (= ordinal-one ordinal-two) (string= kind-one kind-two))
           (cons (cons (if (string= kind-one "code")
                           'code
                         'docs)
                       ordinal-one)
                 keywords)
         (error "Chunk nesting error encountered."))))
(ordinal (substring [0-9] (* [0-9]))
         `(number -- (string-to-number number)))
(kind (substring (or "code" "docs")))
@

Valid [[chunk-contents]] is somewhat confusing, because chunks
can contain many types of information other than text and new
lines. The definition of what is valid follows.

\begin{enumerate}
\item \texttt{text}
\item \texttt{nl}
\item \texttt{defn \textit{name}}
\item \texttt{use \textit{name}}
\item \texttt{line \textit{n}}
\item \texttt{language \textit{language}}
\item \texttt{index \ldots}
\item \texttt{xref \ldots}
\end{enumerate}

Any other keywords are invalid inside a code block. An example of an
invalid keyword is anything related to quotations! \textit{This
restriction only applies to code blocks, however, and documentation
chunks may contain quotations, of course.} As an exception, the
keywords were originally banned inside code chunks, but to parse the
noweb document in which \whyse{} itself was written it needed to be
adjusted. The grammar should be studied again to ensure that textual
description and reality are in step.

%% TODO: rephrase this and ensure it is accurate. The rules for parsing
%% were modified quite significantly from the initial definition so that it
%% would ``work'', and so that \textit{whyse.nw} would be successfully
%% parsed. ``Further, the implementation was modified to accommodate the
%% parsing of the Noweb \whysenw{} firstly.''

<<chunks and their boundaries>>=
(chunk-contents
 (or
  <<structural keywords>>
  <<tagging keywords>>
  x-notused
  <<tool errors>>))
@

It is easier to handle the fatal keyword appearing inside chunks when it
is a permissible keyword to appear inside a chunk; this allows the
parser to consider a chunk with fatal inside of it \textit{as a valid
chunk}, but that does not mean that a chunk with a fatal keyword
inside it does not invalidate a Noweb, it still does: the fatal keyword
causes a fatal crash in parsing regardless. Those structural keywords
which may be used inside the contents of a chunk are given next.

<<structural keywords (except quotations)>>=
;; structural
text
nwnl ;; Noweb's @nl keyword, as differentiated from the rule nl := "\n".
defn
;;; NOTE: previously, a note on the following line incorrectly stated
;;; the `use' token was related to the `identifierusedinmodule' table,
;;; when it is actually related to the `parentchild' table.
use
@

All structural keywords, then, are:

<<structural keywords>>=
<<structural keywords (except quotations)>>
quotation
@

<<tagging keywords>>=
;; tagging
line
language
;; index
i-define-or-use
i-definitions
;; xref
x-prev-or-next-def
x-continued-definitions-of-the-current-chunk
i-usages
x-usages
x-label
x-ref
@

Sometimes, however, the system will permit a chunk to be undefined and
this leads to the only cases in the tool syntax where it is not
line-oriented. [[noidx]] will read the cross references to other chunks
and will be unable to generate the label, so it will insert [[@notdef]]
where it would otherwise upcase ``nw'' and then insert the label. This
is why [[x-undefined]] is placed among the other [[<<tool errors>>]]
keywords.

<<tool errors>>=
;; error
fatal
x-undefined
@

The fundamental keywords are ``text'' and ``nwnl'' (new line, per Noweb
convention). Text keywords contain source text, and any new line tokens
in the source text are replaced with the appropriate number of @nl
keywords (per convention); these are reduced to a single text token when
they are adjacent on the [[peg--stack]].

<<chunks and their boundaries>>=
(text (bol) "@text" spc (substring (* (and (not "\n") (any)))) nl
      `(txt -- (peg-noweb-concatenate-text-tokens (cons 'text txt))))
(nwnl (bol) (substring "@nl") nl (action (setf w--file-current-line
                                               (1+ w--file-current-line)))
      ;; Be sure that when thinking about the symbol `nl' here that
      ;; you're not confusing it with the peg rule nl.
      `(nl -- (peg-noweb-concatenate-text-tokens (cons 'nl "\n"))))
@

Nowebs are built from chunks, so the definition and usage of (i.e.
references to) a chunk are important keywords.

<<chunks and their boundaries>>=
(defn "@defn" spc (substring !eol) nl
      `(name -- (cons 'chunk name)))

;; In @<<whyse.el>>=, it leads to usages tokens like below:
;; (chunk-child-usage . "Commentary")
;; (chunk-child-usage . "Code")
(use
 (bol) "@use" spc (substring !eol) nl
 `(name --
        (if name (cons 'chunk-child-usage name)
          (error "UH-OH! There's a syntax error in the tool output!"))))
@

\begin{quotation}
Documentation may contain text and newlines, represented by @text and
[@nwnl]. It may also contain quoted code bracketed by @quote . . .
@endquote. Every @quote must be terminated by an @endquote within the
same chunk. Quoted code corresponds to the [[â¦]] construct in
the noweb source.
\end{quotation}

<<quotations>>=
(quotation (bol) "@quote" nl
           (action (when w--peg-parser-within-codep
                     (error "The parser found a quotation within a code chunk. A @fatal should have been found here, but was not.")))
           (substring (+ (and (not "@endquote") (any))))
           (bol) "@endquote" nl
           `(lst -- (cons 'quotation lst)))
@

<<keyword definitions>>=
(line (bol) "@line" spc (substring ordinal) nl
      `(o -- (cons 'line o)))

(language (bol) "@language" spc (substring words-eol))
@

The indexing and cross-referencing abilities of Noweb are excellent
features which enable a reader to navigate through a printed (off-line)
or on-line version of the literate document quite nicely. These
functionalities each begin with a rule which matches only part of a line
of the tool syntax since there are many indexing and cross-referencing
keywords. The common part of each line is a rule which merely matches
the [[@index]] or [[@xref]] keyword. The rest of the lines are handled
by a list of rules in [[index-keyword]] or [[xref-keyword]].


The \textit{Noweb Hacker's Guide} lists these two lines in the ``Tagging
keywords'' table, indicating that it's unlikely (or forbidden) that the
index or xref keywords would appear alone without any subsequent
information on the same line.

\begin{quotation}
@index ... Index information.

@xref ... Cross-reference information
\end{quotation}

There are many keywords defined by the Noweb tool syntax, so they
are referenced in this block and defined and documented
separately. Some of these keywords are delimiters, so they are
not given full ``keyword'' status (defined as a PEX rule) but
exist as constants in the definition of a rule that defines the
grouping.

<<keyword definitions>>=
;; Index
<<indexing and cross-referencing set-off words>>
<<fundamental indexing keywords, which are restricted to within a code chunk>>
<<the index of identifiers>>
<<unsupported indexing keywords>>

;; Cross-reference
<<cross-referencing keywords>>

;; Error
<<error-causing keywords>>
@

Further keywords are categorized neatly as Indexing or
Cross--referencing keywords, so they are contained in subsections.

\subsection{indexing}
Indexing keywords, both those used within chunks and those used outside
of chunks, are defined in this section. The
[[<<fundamental indexing keywords, which are restricted to within a code chunk>>]],
index definitions or usages of identifiers and track the definitions of
identifiers in a chunk and the usages of identifiers in a chunk. They
may seem redundant, but are not; the Noweb Hacker's Guide offers a
better explanation of the differences.

<<indexing and cross-referencing set-off words>>=
(idx (bol) "@index" spc)
(xr (bol) "@xref" spc)

<<fundamental indexing keywords, which are restricted to within a code chunk>>=
(i-define-or-use
 idx
 (substring (or "defn" "use")) spc (substring !eol) nl
 (action
  (unless w--peg-parser-within-codep
    (error "WHYSE parse error: index definition or index usage occurred outside of a code chunk.")))
 `(s1 s2 -- (cons (make-symbol s1) s2)))

<<identifiers defined in a chunk>>
<<identifiers used in a chunk>>

<<identifiers defined in a chunk>>=
(i-definitions idx "begindefs" nl
               (list (+ (and (+ i-isused) i-defitem)))
               idx "enddefs" nl
               `(definitions -- (cons 'definitions definitions)))
(i-isused idx (substring "isused") spc (substring label) nl
          `(u l -- (cons 'used! l)))
(i-defitem idx (substring "defitem") spc (substring !eol) nl
           `(d i -- (cons 'def-item i)))

<<identifiers used in a chunk>>=
(i-usages idx "beginuses" nl
          (list (+ (and (+ i-isdefined) i-useitem)))
          idx "enduses" nl
          `(usages -- (cons 'usages usages)))
(i-isdefined idx (substring "isdefined" spc label) nl)
(i-useitem idx (substring "useitem" spc !eol) nl) ;; !eol :== ident
@

The summary index of identifiers is a file--specific set of keywords.
The index lists all identifiers defined in the file (at least all of
those recognized by the autodefinitions filter).

<<the index of identifiers>>=
(i-identifiers idx "beginindex" nl
               (list (+ i-entry))
               idx "endindex" nl
               `(l -- (cons 'i-identifiers l)))
(i-entry idx "entrybegin" spc (substring label spc !eol) nl
         (list (+ (or i-entrydefn i-entryuse)))
         idx "entryend" nl
         `(entry-label lst -- (cons 'entry-label lst)))
(i-entrydefn idx (substring "entrydefn") spc (substring label) nl
             `(defn label -- (cons 'defn label)))
(i-entryuse idx (substring "entryuse") spc (substring label) nl
            `(use lst -- (cons 'use lst)))
@

The following chunk's name is documentation enough for the purposes of
\whyse{}. See the Noweb Hacker's Guide for more information.

[[@index nl]] was deprecated in Noweb 2.10, and [[@index localdefn]] is
not widely used (assumedly) nor well-documented, so it is unsupported by
\whyse{} (contributions for improved support are welcomed).

<<unsupported indexing keywords>>=
;; @index nl was deprecated in Noweb 2.10, and @index localdefn is not
;; widely used (assumedly) nor well-documented, so it is unsupported by
;; WHYSE (contributions for improved support are welcomed).
(i-localdefn idx "localdefn" spc !eol nl)
(i-nl idx "nl" spc !eol nl
      (action (error <<index nl error message>>)))
@

\subsection{cross referencing}
<<cross-referencing keywords>>=
(x-label xr (substring "label" spc label) nl
         `(substr -- (cons 'x-label (cadr (split-string substr)))))
(x-ref xr (substring "ref" spc label) nl
       `(substr --  (cons 'ref (cadr (split-string substr)))))

;; FIXME: improve the error handling at this point. It is not fragile
;; any longer, because most things are ignored and this is hackish;
;; however, the message reporting is not too helpful. It would be nice
;; to have _only_ the chunk name reported, and formatted with @<< and >>.
;;
;; TODO: Reproduction steps: make a reference to an undefined code chunk
;; within another code chunk. For fixing this issue, undefined code
;; chunks should also be referenced within quotations in documentation.
(x-undefined
 xr (or "ref" "chunkbegin") spc
 (guard
  (if (string= "nw@notdef"
               (buffer-substring-no-properties (point) (+ 9 (point))))
      (error (format "%s: %s: %s:\n@<@<%s>>"
                     "WHYSE"
                     "nw@notdef detected"
                     "an undefined chunk was referenced"
                     (buffer-substring-no-properties (progn (forward-line) (point))
                                                     (end-of-line)))))))

(x-prev-or-next-def
 xr (substring (or "nextdef" "prevdef")) spc (substring label) nl
 `(previous-or-next-chunk-defn label -- (cons (make-symbol previous-or-next-chunk-defn) label)))

(x-continued-definitions-of-the-current-chunk
 xr "begindefs" nl
 (list (+ (and xr (substring "defitem") spc (substring label) nl)))
 xr "enddefs" nl)

(x-usages
 xr "beginuses" nl
 (list (+ (and xr "useitem" spc (substring label) nl)))
 xr "enduses" nl)

(x-notused xr "notused" spc (substring !eol) nl
           `(name -- (cons 'unused! name)))
(x-chunks nwnl
          nwnl
          xr "beginchunks" nl
          (list (+ x-chunk))
          xr "endchunks" nl
          `(l -- (cons 'x-chunks l)))
(x-chunk xr "chunkbegin" spc (substring label) spc (substring !eol) nl
         (list (+ (list (and xr
                             (substring (or "chunkuse" "chunkdefn"))
                             `(chunk-usage-or-definition -- (make-symbol chunk-usage-or-definition))
                             spc
                             (substring label)
                             nl))))
         xr "chunkend" nl)

;; Associates label with tag (@xref tag $LABEL $TAG)
(x-tag xr "tag" spc label spc !eol nl)
(label (+ (or "-" [alnum]))) ;; A label never contains whitespace.

<<error-causing keywords>>=
;; User-errors (header and trailer) and tool-error (fatal)
;; Header and trailer's further text is irrelevant for parsing, because they cause errors.
(header (bol) "@header" ;; formatter options
        (action (error "[ERROR] Do not use totex or tohtml in your noweave pipeline.")))
(trailer (bol) "@trailer" ;; formatter
         (action (error "[ERROR] Do not use totex or tohtml in your noweave pipeline.")))
(fatal (bol) "@fatal"
       (action (error "[FATAL] There was a fatal error in the pipeline. Stash the work area and submit a bug report against Noweb, WHYSE, and other relevant tools.")))
@

<<index nl error message>>=
(string-join
 '("\"@index nl\" detected."
   "This indicates hand-written @ %def syntax in the Noweb source."
   "This syntax was deprecated in Noweb 2.10, and is entirely unsupported."
   "Write an autodefs AWK script for the language you are using.")
 "\n")
@

%%  NOTE: it would be helpful to construct this sort of parse tree at
%%  the CHUNK level, and this information can be directly sent to the
%%  database.
%% `((n . ,n)
%%   (name . ,substr)
%%   ;; Displacement: count of @nl encountered in this file so far.
%%   (offset . ,offset)
%%   (file . ,file)
%%   (section . ,section)) ;; Discover a single LaTeX sectioning command
%%                         ;; in the @text commands which are prior to
%%                         ;; this module's definition, as that is the
%%                         ;; direct parent section of this module.
%%  SQL attributes in the `module' table
%% module_number; module_name; displacement; file_name; section_name

%% The definition of a Noweb file, given by Ramsey, is simply a file
%% containing one or more chunks; minimally, a Noweb file will contain the
%% default documentation chunk.

%% To summarize this section, since it is longer than the previous section,
%% the object is to convert the noweb document to tool syntax and parse it
%% with the peg parser.

%% In more explicit words, this section describes the actions that occur
%% when a user invokes [[whyse]] interactively (with \textit{M-x}) and the
%% preconditions have been met; the [[whyse]] function has already been
%% introduced, and only the ``meaty'' business end of its operation has
%% been left undefined until now. Ergo, [[w-with-project]] gathers together
%% the functionality that converts a Noweb to its tool syntax with a
%% project's specified shell script, and parses the text before the next
%% section of body forms is executed. Those send the parsed text to the
%% database, and finally create the atomic window for the IDE in the active
%% frame.

%% In earlier development versions the following function body was
%% referenced with noweb chunks rather than being defined as a function,
%% however it was decided that implementing this as a function allowed a
%% user-suggestion for an API-like function that was planned for
%% implementation anyways. Now it is a function, satisfying internal and
%% API needs.

<<parse-project-in-temp-buffer>>=
(with-temp-buffer
  (insert (shell-command-to-string (w-project-script project)))
  (goto-char (point-min))
  (w--parse-current-buffer-with-rules))

<<with-project>>=
;; TODO: understand how and why this macro works like
;; `with-temp-buffer'; I only copied a couple things from the defintion
;; of that macro, but I don't yet understand its definition fully. I
;; will need to before I really start to understand Emacs Lisp. I do
;; understand enough, but there is a lot more to know. A whole lot.
(defmacro w-with-project (project &rest body)
  (declare (indent 0) (indent 2) (debug t))
  "Evaluates BODY with PROJECT in scope. This is like `let' except
it doesn't bind arbitrary values to arbitrary symbols. PROJECT is
taken as the symbol `project' during evaluation of BODY.

PROJECT is in scope because it is an argument of this macro. This
may or may not work as expected."
  (eval `(progn ,@body)))

<<buffer parsing function>>=
;; FIXME: the current parse tree contains a `nil' after the chunk type
;; and number assoc, and that needs to be analyzed. Why is this `nil' in
;; the stack? I assume and believe it is because of the collapsing of
;; stringy tokens; when a token should be put back onto the stack it may
;; also be putting a `nil' onto the stack in the first call to the
;; function.
;;;; Parsing expression grammar (PEG) rules
(defun w--parse-current-buffer-with-rules ()
  "Parse the current buffer with the PEG defined for Noweb tool syntax."
  (with-peg-rules (<<PEG rules>>)

   ;; FIXME: within this local, temporary scope there is no reason to use `w--' in a name.
   (let (w--peg-parser-within-codep
         (peg-noweb-first-stringy-token? t))
     (peg-run (peg noweb)
              (lambda (lst)
                "This lambda was historically named `w--parse-failure-function'."
                (setq w--parse-success nil)
                (pop-to-buffer (clone-buffer))
                (save-excursion
                  (put-text-property (point) (point-min)
                                     'face 'success)

                  (put-text-property (point) (point-max)
                                     'face 'error)

                  (goto-char (point-max))
                  (message "PEXes which failed:\n%S" lst)))))))
@

<<peg-noweb code>>=
(defvar w--parse-success t; a global variable
  "The success or failure of the last parsing of noweb tool syntax.")
@

\chapter{Processing parsed nowebs into SQL}\label{processing lists}
This chapter covers how the parsed text generated in the \ref{parsing}
chapter is processed, either creating a series of \sql{} statements that
will be executed by SQLite using the interface provided by the
\textsc{EmacSQL} package, or used in some other way through \elisp{}.

First, the overall structure of the parsed text should be diagrammed.
The parse tree is a list of noweb documents, each being a list
themselves. The first atom of an inner list, corresonding to a document,
is the filename of that document (hopefully the same filename as passed
on the commmand-line elsewhere when the document is used).

Deeper, each document-list contains as the second atom a list of its
contents, which is an association list thereof. Each association in the
alist is bewteen the symbol relating to the rule that generated the cons
cell, and the contents appropriate to the symbol.

\section{Processing}
There are many steps to compiling the parse tree into \sequel{}. The first
step is to ensure the association lists in the parse tree are in a
format that is acceptable to built-in Emacs Lisp functions; this will
make it easier to navigate the tree and transform it.
%% FIXME: is it really true, and does it make sense for
%% what I'm actually doing, or am I using the concept/name wrong?
Other texts call this manipulation of the parse tree ``list
destructuring'';
%%
it could be thought of as code generation, since the data is used to
generate DML which is partly code and partly data.

%% FIXME 2024-08-10 what the fuck am I saying here?
Some associations are reductions from the initial parse tree,
which---thanks to the \textsc{peg} package and \textit{PEG parsing in
  general}---are not reduced in a second step. Modifications to the
parsing procedure can occur directly without losing information, and
thanks to literate programming should be easy for advanced users.

The initial parse tree begins like the one below.

\begin{verbatim}
'((noweb-document-one
   ((docs . 0)
    (text . "\tex{} is cool!"))
   ((code . 1)
    (text . "(message \"LISP is awesome!\")")))
  (noweb-document-two
   ((code . 0)
    (text . "asdf is a system definition format in Common LISP,"))
   (nwnl . "\n")
   (text . "and I like to use it.")
   ((code . 1)
    (text . "jkl; is the right-handed corollary of asdf."))
   ((docs . 2)
    (text . "\LaTeX{} is great!"))
   ((docs . 3)
    (text . "Noweb, written by Norman Ramsey is sweet!"))))
\end{verbatim}

The reductions which occur during parsing make chunk zero of the second
noweb document look like this in the final result.

\begin{verbatim}
((code . 0)
 (text . "asdf is a system definition format in Common LISP,
and I like to use it."))
\end{verbatim}

The new result is much easier to use as data for other programs (SQL in
this case). In the verbatim text a literal newline was inserted rather
than retaining the escape sequence, which is exactly what happens in the
reduction step as well. The next subsection discusses the details of how
the reduction in complexity exampled above is achieved.

\subsection{Reducing complexity in the alist}
The first step in making the parse tree navigable for other programs is
collapsing adjacent ``stringy'' tokens into single [[text]] tokens. The
output tool syntax of notangle, and the parse tree resulting from the
PEG, (breifly) contain individual text tokens for fragments of whole
text lines and form feed characters. These tokens exist because the
cross-referencing tokens fragment the text lines, and new lines in the
noweb document are treated specially to facilitate this fragmentation.

A small quote from the tool syntax of a development version of \whyse{} is
shown in this example in its parsed form. However, during actual parsing
these adjacent tokens are \textit{immediately collapsed} into singular
tokens.

\begin{verbatim}
(text . "  and \textsc{Noweb}'s \texttt{finduses.nw}!")
(nwnl . "@nl")
(text . "\end{enumerate}")
(nwnl . "@nl")
(text . "")
(nwnl . "@nl")
\end{verbatim}

To collapse these tokens into a single text token the [[peg--stack]]
must be manipulated carefully. It isn't advisable to manipulate this
variable in the course of a PEG grammar's actions. There is a use case
for it when the previous rules and actions won't accommodate the
necessary action without refactoring a larger part of the grammar. In
this development version that is not a goal; basic functionality is
sought after, not robustness or beauty, so hacking the desired behaviour
together quickly is better.

[[peg-noweb-nth-chunk-of-nth-noweb-document]] retrieves the parse tree for the nth
noweb document, which in the case of [[whyse.nw]] is the parse tree of
the zeroth-indexed document. It's quite a simple function. To obtain a
given chunk of this document from the parse tree the result of the
function is called with [[nth]] and the index of the chunk.

<<functions for navigating \whyse{} parse trees>>=
(defun peg-noweb-nth-document-file-name (nth-document parse-tree)
  "Return the file name of the nth-indexed document in the parse tree.

For the first document in the parse tree, that is the
zeroth-indexed document."
  (cl-first (nth nth-document parse-tree)))

(defun peg-noweb-nth-document (nth-document parse-tree)
  "Return the subtree of the nth-indexed document in the parse tree."
  (cl-second (nth nth-document parse-tree)))

(defun peg-noweb-nth-chunk-of-document (n document)
  "Return the subtree for the Nth chunk of a noweb document parse subtree."
  (nth n document))

(defun peg-noweb-chunk-number (chunk)
  "Return the chunk number of CHUNK."
  (or (cdr (assq 'code chunk))
      (cdr (assq 'docs chunk))))

(defun peg-noweb-nth-chunk-of-nth-noweb-document (nth-chunk nth-document parse-tree)
  (peg-noweb-nth-chunk-of-document nth-chunk (peg-noweb-nth-document nth-document parse-tree)))

(defun peg-noweb-chunk-text (chunk)
  "Join all the strings returned from the collection in the loop,
and return the single string."
  (string-join
   (cl-loop for elt in chunk collect
            (when (and (listp elt) (equal 'text (car elt)))
              (cdr elt)))
   ""))

(defun peg-noweb-chunk-name (chunk)
  "Return non-nil if CHUNK is a code chunk, and thereby has a name.

The return value, if non-nil, is actually the name of the chunk."
  (if-let ((name (assq 'chunk chunk)))
      (cdr name)))

<<functions to collapse text and newline tokens into their largest possible form>>=
(defun peg-noweb-concatenate-text-tokens (new-token)
  "Join the values of two text token associations in a two-element token alist.

If the two associations shouldn't be joined, return them to the stack."
  (prog1
      ;; Concatenation only occurs when the previous token examined was
      ;; a text or nwnl token, ergo there must have been a text or nwnl
      ;; token previously examined for any concatenation to occur. When
      ;; no such token has been examined immediately return the
      ;; (stringy) token recieved and indicate it must have been a
      ;; stringy token by chaning the value of `peg-noweb-first-stringy-token?'
      ;; accordingly. Subsequent runs will then operate on potential
      ;; pairs of stringy tokens.
      (if-let ((not-first-stringy-token? (not peg-noweb-first-stringy-token?))
               (previous-token  (pop peg--stack))
               ;; The previous token cannot be a text or nwnl token if
               ;; it is not a list, and checking prevents causing an
               ;; error by taking the `car' of a non-list token, e.g. the
               ;; filename token.
               (previous-token-is-alist?
                (prog1 (and (listp previous-token)
                            (listp new-token)
                            (or (assoc 'text `(,new-token))
                                (assoc 'nl `(,new-token)))
                            (or (assoc 'text `(,previous-token))
                                (assoc 'nl `(,previous-token)))))))
          ;; Join the association's values and let the caller push a single
          ;; token back onto the `peg--stack'.
          (cons 'text (format "%s%s" (cdr previous-token)
                              (cdr new-token)))

        ;; Push the previous token back to the `peg--stack', and let the
        ;; caller push the new token to that stack.
        (push previous-token peg--stack)
        new-token)
    (when peg-noweb-first-stringy-token? (setq peg-noweb-first-stringy-token? nil))))

<<peg-noweb code>>=
<<functions for navigating peg-noweb parse trees>>
<<functions to collapse text and newline tokens into their largest possible form>>

@

The `parse-tree' from the lexical environment (or scope, given that
peg-noweb is not implemented with lexical binding) is mapped over until
every file token has been processed and records have been inserted into
all four tables of the database.

All files represented in the [[parse-tree]] (actually an association
list) are processed into SQL database records while mapping over the
[[parse-tree]] (whichever is in scope at the time because peg-noweb is
implemented without lexical binding).

<<peg-noweb code>>=
(defun peg-noweb-prepare-sexp-sql-from-file-tokens ()
  "Prepare an s-expression of SQL statements for `emacsql'.

The `parse-tree' from the lexical environment (or scope, given
that WHYSE is not implemented with lexical binding) is mapped
over until every file token has been processed and records have
been inserted into all four tables of the database.

This hook depends on this object being in scope: `parse-tree'.
That object is in scope when this hook runs with the default
implementation of `whyse'. (This is not meant to imply other
implementations exist [yet], only that hacked up installations
won't operate with any guarantees or according to the original
documentation. You know that, though, because you're reading the
source code and documentation for the original right now, and it
should be obvious that changing the source code should have been
accompanied with changes in the documentation.)"
  (mapcar
   (lambda (file-token)
     (let* ((file-name (car file-token))
            (chunks (cdr file-token))
            (modules (vector
                      :insert :into 'module
                      :values (mapcar #'peg-noweb-vectorize-chunk-data chunks)
                      :on-conflict-do-nothing))
            (parent-child (vector
                           :insert :into 'parentchild
                           :values (peg-noweb-parent-child-relationships)
                           :on-conflict-do-nothing))
            (connection (w-project-database-connection project)))
       (mapcar (lambda (seql) (emacsql connection seql))
               (list modules
                     parent-child
                     ;; TODO:
                     ;; identifier-used-in-module
                     ;; topic-referenced-in-module
                     ))))
   parse-tree))

(defun peg-noweb-vectorize-chunk-data (chunk)
  "Convert an individual chunk to a vector of objects.

NOTE: order of arguments: [module-name module-text file-name
section displacement module-number]"
  (let ((chunk-name (peg-noweb-chunk-name chunk)))
    (vector chunk-name
            (peg-noweb-chunk-text chunk)
            file-name
            (when chunk-name
              (peg-noweb-chunk-section chunk-name))
            nil
            (peg-noweb-chunk-number chunk))))

(defun peg-noweb-parent-child-relationships ()
  "Make a list of vectors of parent-child relationships.

The elements are taken from applying `peg-noweb-get-chunk-uses-of-chunks' to
every chunk in the noweb."
  (apply #'append (cl-mapcar #'peg-noweb-get-chunk-uses-of-chunks chunks)))

(defun peg-noweb-get-chunk-uses-of-chunks (chunk)
  "Make a list of vectors of parent-child relationships.

The parent is CHUNK, and the first element of each vector in the
list will be the number of this chunk. The chunks this chunk uses
will be named in the vectors in the returned list."
  (with-temp-buffer
    (let* ((iter 0)
           (parent-uses-child (list nil))
           (chunk-number (assoc-default 'code chunk))
           (chunk-formatted (format "%S" chunk)))
      (insert chunk-formatted)
      (goto-char (point-min))
      (cl-remove
       nil
       (cl-mapcar (lambda (assoc)
                    (when (equal (car assoc) 'chunk-child-usage)
                      (vconcat (list (number-to-string chunk-number)
                                     (cdr assoc)))))
                  chunk)))))

@

When the SEQL (S-EXP SQL) has been compiled, it should be logged. This
might be better controlled with DCL (the data control langauge
featureset of SQL). The history of SEQL statements compiled is treated
like a stack, with the newest statement being the first element of the
list. A shoddy implementation of a ``stack-based history'' is given
next.

<<push the compiled SQL to the database and to the history stack>>=
;; NOTE: the result of evaluating the SQL is pushed to the history stack
;; alongside the SQL that was executed.
(cl-pushnew (cons (emacsql (w-project-database-connection default-project)
                           compiled-parse-tree)
                  . compiled-parse-tree)
            (w-project-history-sql-commands default-project))
@





\subsection{finding the section title of a chunk}
<<Code>>=
<<find section title>>

<<find section title>>=
;; TODO 2024-05-13: this function is really rough. It still needs a
;; working algorithm to search through the noweb text to find the
;; section that a chunk belongs to. I think that searching for all of
;; the sections that preceed a chunk definition will be necessary before
;; finding the last section command that preceeds a chunk (thereby
;; finding the section the chunk belongs to). There may be a better
;; algorithm; this is the first algorithm I thought of to find the
;; section that a chunk belongs to regardless of the regular expression
;; used.
;;
;; Another algorithm might use Emacs Lisp regular expressions to search
;; for the first match to any of the set of regular expressions that
;; would search for a given sectioning command.
;;
;; For now, I trust the Emacs Lisp manual when it says that `regex-opt'
;; is efficient; it is damned convenient right now.
(defun peg-noweb-chunk-section (chunk-name)
  (with-temp-buffer
    (insert-file-contents (w-project-noweb project))
    (goto-char 0)
    (re-search-forward
     ;; It is an error if no chunks are found.
     (regexp-opt (list (concat "<" "<" chunk-name ">" ">" "="))))
    (re-search-backward
     ;; TODO: support a fallback list from RefTeX.
     (regexp-opt (w-project-sectioning-commands-regexs project))
     ;; It is not an error if no section is found because sectioning is
     ;; entirely optional in a literate document with LaTeX.
     nil t)
    (buffer-substring (line-beginning-position) (line-end-position))))
@

While it's certain that some of the data from a search operation are
markers of a match (some of them are non-nil), substrings corresponding
to the used chunk name in the current chunk are appended to a list that
is ultimately returned.

\subsection{Determining parent-child relationships}
There needs to be a function which can take a generalizable approach to
determining the name of a section defined by any possible sectioning
command. To easily do that a function could be provided by the user for
any sectioning commands that are exotic, and those functions will
accommodate the LaTeX command while the calling function will handle
finding them and collecting their output.

%% NOTE: There may be some semantic or logical connection between the
%% parent-child chunk relationship and the latex sections which contain
%% parent chunks, or even child chunks! What is that?

The following association list gives the keyword and the value
corresponding to that which were seen in the intermediate representation
used by noweb itself. To implement \whyse{} we utilize the intermediate
representation (IR) for both editing and document compilation. Features
unique to \whyse{} would add in new keywords, and these would only be
processed by \whyse{}; filters should always ignore IR content that
doesn't concern them.

\begin{verbatim}
((code . 32)
 (x-label . "NW2ucZJx-3LzQog-2")
 (ref . "NW2ucZJx-3LzQog-1")
 (chunk . "chunks and their")% ELIDED
 (prevdef . "NW2ucZJx-3LzQog-1")
 (nextdef . "NW2ucZJx-3LzQog-3")
 (text . "\n(chunk-contents\n...")% ELIDED
 (x-label . "NW2ucZJx-3LzQog-2-u1")
 (ref . "NW2ucZJx-2ZphFz-1")
 (chunk-child-usage . "structural keywords")
 (text . "\n  ")
 (x-label . "NW2ucZJx-3LzQog-2-u2")
 (ref . "NW2ucZJx-44iV1g-1")
 (chunk-child-usage . "tagging keywords")
 (text . "\n  x-notused\n  ")
 (x-label . "NW2ucZJx-3LzQog-2-u3")
 (ref . "NW2ucZJx-1ZiKLq-1")
 (chunk-child-usage . "tool errors")
 (text . "))\n"))
\end{verbatim}

<<collect child chunk uses>>=
(with-temp-buffer
  (let* ((iter 0)
         (parent-uses-child (list nil))
         (chunk-number (assoc-default 'code chunk))
         (chunk-formatted (format "%S" chunk)))
    (insert chunk-formatted)
    (goto-char (point-min))
    (cl-remove
     nil
     (cl-mapcar (lambda (assoc)
                  (when (equal (car assoc) 'chunk-child-usage)
                    (vconcat (list chunk-number (cdr assoc)))))
                chunk))))
@
